"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[3589],{8437:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>i});const r=JSON.parse('{"id":"storage/manage-storage","title":"Manage storage","description":"Expand Rook-Ceph storage","source":"@site/docs/80-storage/20-manage-storage.md","sourceDirName":"80-storage","slug":"/storage/manage-storage","permalink":"/nursery/storage/manage-storage","draft":false,"unlisted":false,"editUrl":"https://github.com/infaloom/nursery/tree/main/docusaurus/docs/80-storage/20-manage-storage.md","tags":[],"version":"current","sidebarPosition":20,"frontMatter":{},"sidebar":"defaultSidebar","previous":{"title":"Rook Ceph Cluster","permalink":"/nursery/storage/rook-ceph-cluster"},"next":{"title":"Cluster Roles & Secrets","permalink":"/nursery/cnpg/cluster_roles_and_secrets"}}');var t=n(4848),s=n(8453);const a={},l="Manage storage",c={},i=[{value:"Expand Rook-Ceph storage",id:"expand-rook-ceph-storage",level:2},{value:"Expanding PVCs",id:"expanding-pvcs",level:2},{value:"Shrink Rook-Ceph storage",id:"shrink-rook-ceph-storage",level:2}];function d(e){const o={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(o.header,{children:(0,t.jsx)(o.h1,{id:"manage-storage",children:"Manage storage"})}),"\n",(0,t.jsx)(o.h2,{id:"expand-rook-ceph-storage",children:"Expand Rook-Ceph storage"}),"\n",(0,t.jsx)(o.p,{children:(0,t.jsx)(o.a,{href:"https://rook.io/docs/rook/v1.9/ceph-osd-mgmt.html",children:"https://rook.io/docs/rook/v1.9/ceph-osd-mgmt.html"})}),"\n",(0,t.jsx)(o.p,{children:"Initially, each agent node in the Kubernetes cluster will have 2 volumes of 100GB each. 6 volumes total.\r\nYou can increase this number to 3 per node or any other number of volumes by updating the Pulumi code.\r\nWithout additional updates to the code we are always adding the same number of volumes to each agent node."}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-csharp",children:'foreach (var agent in k8sAgentServer)\r\n{\r\n    int numberOfVolumesPerAgent = 3; // <-- UPDATE THIS NUMBER TO ADD MORE VOLUMES\r\n    for (var i = 1; i <= numberOfVolumesPerAgent; i++)\r\n    {\r\n        Output.Format($"{agent.Name}_volume_{i}").Apply(volumeName =>\r\n        {\r\n            var volume = new HCloud.Volume(volumeName, new()\r\n            {\r\n                Name = volumeName,\r\n                Size = 100,\r\n                ServerId = agent.Id.Apply(int.Parse),\r\n            });\r\n            return Task.CompletedTask;\r\n        });\r\n    }\r\n}\n'})}),"\n",(0,t.jsxs)(o.p,{children:["Make sure you run ",(0,t.jsx)(o.code,{children:"pulumi preview"})," before applying the updates. When updating from the default 2 volumes to 3 volumes per node the output should look like this:"]}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{children:"Previewing update (production):\r\n     Type                    Name                   Plan\r\n     pulumi:pulumi:Stack     hetzner-production\r\n +   \u251c\u2500 hcloud:index:Volume  agent1_volume_3        create\r\n +   \u251c\u2500 hcloud:index:Volume  agent2_volume_3        create\r\n +   \u2514\u2500 hcloud:index:Volume  agent3_volume_3        create\r\n\r\nResources:\r\n    + 3 to create\r\n    38 unchanged\n"})}),"\n",(0,t.jsxs)(o.p,{children:["Run ",(0,t.jsx)(o.code,{children:"pulumi up"})," to apply the changes."]}),"\n",(0,t.jsx)(o.p,{children:"In order to create new OSDs in the ceph cluster Rook operator needs to be restarted by deleting the operator pod"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:"kubectl -n rook-ceph delete pod $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-operator\" -o jsonpath='{.items[0].metadata.name}')\n"})}),"\n",(0,t.jsxs)(o.p,{children:["Confirm that the new OSDs have been added to the cluster and are in ",(0,t.jsx)(o.code,{children:"up"})," status. ",(0,t.jsx)(o.strong,{children:"This may take a few minutes!"})]}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:"kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph osd tree\n"})}),"\n",(0,t.jsxs)(o.p,{children:["Confirm that the Ceph cluster is healthy ",(0,t.jsx)(o.code,{children:"health: HEALTH_OK"})," by running"]}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:"kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status\n"})}),"\n",(0,t.jsx)(o.h2,{id:"expanding-pvcs",children:"Expanding PVCs"}),"\n",(0,t.jsx)(o.admonition,{type:"warning",children:(0,t.jsx)(o.p,{children:"It is easy to expand a PVC. Shrinking is not supported. Expand wisely!"})}),"\n",(0,t.jsxs)(o.p,{children:["Let's for example expand the PVC of the test ",(0,t.jsx)(o.code,{children:"whoami"})," workload."]}),"\n",(0,t.jsxs)(o.p,{children:["Update the file ",(0,t.jsx)(o.code,{children:"k8s/whoami/whoami.yaml"})," as follows:"]}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-yaml",children:"kind: PersistentVolumeClaim\r\napiVersion: v1\r\nmetadata:\r\n  name: busybox-logs\r\n  namespace: whoami\r\nspec:\r\n  storageClassName: ceph-block\r\n  accessModes:\r\n    - ReadWriteOnce\r\n  resources:\r\n    requests:\r\n      storage: 100Mi # <- UPDATE THIS VALUE FROM 50Mi to 100Mi\n"})}),"\n",(0,t.jsxs)(o.p,{children:["Run ",(0,t.jsx)(o.code,{children:"kubectl apply"})]}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:"envsubst < k8s/whoami/whoami.yaml | kubectl apply --wait -f -\n"})}),"\n",(0,t.jsx)(o.p,{children:"Expansion will happen automatically after a few seconds."}),"\n",(0,t.jsx)(o.h2,{id:"shrink-rook-ceph-storage",children:"Shrink Rook-Ceph storage"}),"\n",(0,t.jsxs)(o.admonition,{type:"danger",children:[(0,t.jsx)(o.p,{children:"Ensure your new configuration has enough space to hold all the data."}),(0,t.jsx)(o.p,{children:"Shrinking storage is a complex operation and may lead to data loss if not done properly."}),(0,t.jsx)(o.p,{children:"It is recommended to backup all important data before proceeding."}),(0,t.jsx)(o.p,{children:"Also ensure that the number of volumes per node doesn't go bellow 2!"}),(0,t.jsx)(o.p,{children:"Always remove one volume at a time per node and wait for the Ceph cluster to stabilize before continuing."})]}),"\n",(0,t.jsxs)(o.p,{children:["Details at ",(0,t.jsx)(o.a,{href:"https://rook.io/docs/rook/v1.9/ceph-osd-mgmt.html#remove-an-osd",children:"https://rook.io/docs/rook/v1.9/ceph-osd-mgmt.html#remove-an-osd"})]}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:"# Stop Rook operator\r\nkubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=0\n"})}),"\n",(0,t.jsx)(o.p,{children:"It is recommended to run the following sequence of commands for each OSD wait for the Ceph cluster to stabilize before continuing on with removals."}),"\n",(0,t.jsx)(o.admonition,{type:"note",children:(0,t.jsx)(o.p,{children:"OSD IDs are zero based. Initial 6 OSDs they will be numbered from 0 to 5.\r\nIf you are removing the last volume from each node, the OSD IDs will be 6, 7, 8 etc."})}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:"# Stop OSD deployment\r\nkubectl -n rook-ceph scale deployment rook-ceph-osd-{ID} --replicas=0\r\n# Mark OSD as down (may report that the OSD is already down)\r\nkubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph osd down osd.{ID}\r\n# Mark OSDs as out\r\nkubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph osd out osd.{ID}\r\n# Wait for Cepth to finish backfilling to other OSDs\r\nkubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status\r\n# Purge OSDs\r\nkubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph osd purge {ID} --yes-i-really-mean-it\r\n# Confirm they are removed\r\nkubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph osd tree\r\n# Remove OSD deployment\r\nkubectl delete deployment -n rook-ceph rook-ceph-osd-{ID}\n"})}),"\n",(0,t.jsx)(o.p,{children:"Update Pulumi code by reducing the number of volumes. Always remove 1 volume per node per session to allow Ceph cluster to stabilize."}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-csharp",children:"int numberOfVolumesPerAgent = 2;\n"})}),"\n",(0,t.jsx)(o.p,{children:"Update the infrastructure"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:"# Preview changes\r\npulumi preview\r\n# Apply updates\r\npulumi up\n"})}),"\n",(0,t.jsx)(o.p,{children:"Start Rook operator"}),"\n",(0,t.jsx)(o.pre,{children:(0,t.jsx)(o.code,{className:"language-bash",children:"kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=1\n"})})]})}function h(e={}){const{wrapper:o}={...(0,s.R)(),...e.components};return o?(0,t.jsx)(o,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,o,n)=>{n.d(o,{R:()=>a,x:()=>l});var r=n(6540);const t={},s=r.createContext(t);function a(e){const o=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function l(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),r.createElement(s.Provider,{value:o},e.children)}}}]);